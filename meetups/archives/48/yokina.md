# yokina(kaeruko)

## 会社や業務で普段やっていること

- スマートロックのバックエンド(rails,django,go)

## (option) 相談乗れるかもしれないこと

- php,rails

## 今日やること

- ゼロから作るdeep learning ❷ ―自然言語処理編を読む
自然言語処理に興味があります。数式難しい

## (option) もしかしたら相談するかもしれないこと

- 自然言語処理、機械学習

## 今日の成果

今日学習したこと


## 推論
重みと入力を掛け算して、<u>活性化関数</u>を通して出力したパラメータを非線形にし次のレイヤーのforwardに渡す。

最後の出力層のニューロンの値が最も大きい値を探すこと


## 活性化関数
線形を非線形にする関数
(線形だとどこからがいい数字でどこからが悪い数字かとかが区別つかないから…？)

- シグモイド関数

0~1の間に収まり、微分してもゼロにならない。

hの平均くらいのパラメータの変化が大きくて、

平均から遠いと大きくはみ出てもあまり変化がない



## 学習
<u>損失関数</u>が少ない重みを出すこと


## 損失関数
交差エントロピー誤差など

```
-np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
```

softmaxで確率として出力したものと、正解ラベルを計算して
正解の確率が高ければ0に近く、低ければ1に近くなる

## softmax関数
出力したパラメータを0~1の範囲に変換し、確率として使えるようにしたもの


**活性化関数レイヤーも損失関数レイヤーもaffinレイヤーもforward,backwordを実装する**

隠れ層(中間層)のニューロンの数
p8に「ここではひとまず4つのニューロンを置くことにしましょう」とあって
その後ずっと4つなんだけど、どういう基準で決めるんだろう

# 自然言語処理におけるニューラルネットワーク


## 単語の分散表現
単語をベクトルとしてとらえたもの
例：オバマ大統領:[-001.1, 0002.731, 0.34]

## カウントベースの手法
**単語の意味は周囲の単語によって形成される**という仮説に基づいたもの

ある単語に着目した場合、周囲にどのような単語がどれだけ現れるかをカウントした手法

## 推論ベースの手法

### CBOWモデル

**I say hello and You say goodbay**
というコンテキストを与え、

**I** と **hello** という入力を与えた時 **say** を出すモデルを作ること

### Skip-gramモデル

**say** という入力を与えた時 **I** と **hello** を出すモデルのこと
CBOWよりも単語の分散表現の精度が高い

# 高速化

## Embeddingレイヤ
MatMulレイヤーで行っていた正解データと重みの行列の積を行わずに、単語IDの行だけ抜き出したもの

# 多値分類から二値分類に

正解か不正解かを出力する

ニューラルネットワークが正解を出してくる時のスコア(確率)を出来るだけ高く、

不正解を出す時に出してくる確率を出来るだけ0に近づけること

# RNN

これまでの一方通行のニューラルネットワークを再帰的に繰り返し、時系列の概念を足したもの。

コンテキストにおける単語を左から順番に損失関数を出していき、

通り過ぎたレイヤーの損失関数をすべて保存することで

これまでの文脈が理解できる…らしい

